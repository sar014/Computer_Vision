{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Avg Speed and Vehicle Detection\n",
        "\n",
        "### Workflow\n",
        "1. Read frames sequentially from the video stream.\n",
        "2. Resize frames for consistent processing.\n",
        "3. Detect vehicles using YOLO object detection.\n",
        "4. Extract bounding boxes of detected vehicles.\n",
        "5. Track vehicles to obtain persistent IDs and centroids.\n",
        "6. Monitor centroid movement across virtual lines.\n",
        "7. Measure time taken to travel between lines.\n",
        "8. Compute individual vehicle speeds.\n",
        "9. Calculate average speed for traffic analysis.\n",
        "10. Determine congestion level from average speed.\n",
        "11. Visualize detections, speeds, and congestion on frames."
      ],
      "metadata": {
        "id": "H4haSPsoA3ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics opencv-python pandas numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gz4e5AZ2Bau8",
        "outputId": "9fee6c80-2dee-4242-8967-60cf6dac2684"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.4.14-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.13.0.92)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu128)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (26.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2026.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.4.14-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.4.14 ultralytics-thop-2.0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "wv-hdINILbLe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import time\n",
        "from ultralytics import YOLO\n"
      ],
      "metadata": {
        "id": "dS0Cqtr3Dqd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec60414-079a-4362-91a6-cc914925e64e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(\"yolov8n.pt\")   # nano = fast for Colab\n"
      ],
      "metadata": {
        "id": "H470gwR0DvnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80416d36-24d4-49c8-ffce-7e06f0c04cc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 342.2MB/s 0.0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "QYKuOJUrD6Qm",
        "outputId": "bee6ed6c-a33d-4dce-d35c-1854cfbd3789"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c01c957c-13c2-4c1f-a3bc-ec66d5169128\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c01c957c-13c2-4c1f-a3bc-ec66d5169128\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving traffic2.mp4 to traffic2.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tracker:\n",
        "    def __init__(self):\n",
        "        self.next_id = 0\n",
        "        self.objects = {}\n",
        "\n",
        "    def update(self, detections):\n",
        "        updated_objects = {}\n",
        "\n",
        "        for box in detections:\n",
        "            x1, y1, x2, y2 = box\n",
        "            cx = int((x1 + x2) / 2)\n",
        "            cy = int((y1 + y2) / 2)\n",
        "\n",
        "            matched_id = None\n",
        "\n",
        "            for obj_id, (px, py) in self.objects.items():\n",
        "                distance = math.hypot(cx - px, cy - py)\n",
        "                if distance < 35:     # threshold\n",
        "                    matched_id = obj_id\n",
        "                    break\n",
        "\n",
        "            if matched_id is None:\n",
        "                matched_id = self.next_id\n",
        "                self.next_id += 1\n",
        "\n",
        "            updated_objects[matched_id] = (cx, cy)\n",
        "\n",
        "        self.objects = updated_objects\n",
        "        return updated_objects\n",
        "\n"
      ],
      "metadata": {
        "id": "97hfeOB7JdFi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LINE_Y1 = 250   # entry line\n",
        "LINE_Y2 = 320   # exit line\n",
        "PIXEL_DISTANCE = abs(LINE_Y2 - LINE_Y1)\n",
        "\n",
        "PIXEL_TO_METER = 0.05   # adjust depending on camera view\n"
      ],
      "metadata": {
        "id": "0124Qk9fKYrb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tracker = Tracker()\n",
        "\n",
        "entry_time = {}\n",
        "speeds = {}\n",
        "\n",
        "stopped_counter = {}\n",
        "STOP_THRESHOLD_FRAMES = 20\n",
        "MIN_MOVEMENT = 2\n"
      ],
      "metadata": {
        "id": "3q1ZBGa1KZwX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = list(uploaded.keys())[0]\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "print(list(uploaded.keys())[0])\n",
        "\n",
        "frame_id = 0\n"
      ],
      "metadata": {
        "id": "DbfPZHC5Ka3H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24096afc-22dd-471c-c801-79836e7648f1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "traffic2.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "\n",
        "    # Read next frame from the video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Stop if video has ended\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Increment processed frame counter\n",
        "    frame_id += 1\n",
        "\n",
        "    # Resize frame for consistent computation & speed\n",
        "    frame = cv2.resize(frame, (960, 540))\n",
        "\n",
        "    # Run YOLO inference on the frame\n",
        "    results = model(frame)\n",
        "\n",
        "    detections = []   # Store vehicle bounding boxes\n",
        "\n",
        "    # -------- OBJECT DETECTION --------\n",
        "    for r in results:\n",
        "        for box in r.boxes:\n",
        "\n",
        "            # Extract detected class ID\n",
        "            cls = int(box.cls[0])\n",
        "\n",
        "            # Filter only vehicle classes (car, bike, bus, truck)\n",
        "            if cls in [2, 3, 5, 7]:\n",
        "\n",
        "                # Extract bounding box coordinates\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "\n",
        "                # Save detection for tracking stage\n",
        "                detections.append([x1, y1, x2, y2])\n",
        "\n",
        "                # Draw bounding box on frame\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
        "\n",
        "    # -------- TRACKING STAGE --------\n",
        "    # Assign persistent IDs & compute centroids\n",
        "    objects = tracker.update(detections)\n",
        "\n",
        "    # Process each tracked vehicle\n",
        "    for obj_id, (cx, cy) in objects.items():\n",
        "\n",
        "        # Draw centroid of tracked vehicle\n",
        "        cv2.circle(frame, (cx, cy), 4, (0,0,255), -1)\n",
        "\n",
        "        # Display tracking ID\n",
        "        cv2.putText(frame, f\"ID {obj_id}\", (cx, cy-10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)\n",
        "\n",
        "        # -------- SPEED MEASUREMENT LOGIC --------\n",
        "\n",
        "        # Record entry time when centroid crosses first line\n",
        "        if cy > LINE_Y1 and obj_id not in entry_time:\n",
        "            entry_time[obj_id] = time.time()\n",
        "\n",
        "        # Compute speed when centroid crosses second line\n",
        "        if cy > LINE_Y2 and obj_id in entry_time and obj_id not in speeds:\n",
        "\n",
        "            MIN_TIME = 0.3  # Prevent noise / instant jumps\n",
        "\n",
        "            # Time taken between the two lines\n",
        "            elapsed = time.time() - entry_time[obj_id]\n",
        "\n",
        "            if elapsed > MIN_TIME:\n",
        "\n",
        "                # Convert pixel distance → real-world meters\n",
        "                meters = PIXEL_DISTANCE * PIXEL_TO_METER\n",
        "\n",
        "                # Speed formula (m/s → km/h)\n",
        "                speed = (meters / elapsed) * 3.6\n",
        "\n",
        "                # Ignore unrealistic speeds\n",
        "                if 0 < speed < 200:\n",
        "                    speeds[obj_id] = speed\n",
        "\n",
        "        # Display speed if available\n",
        "        if obj_id in speeds:\n",
        "            cv2.putText(frame, f\"{int(speeds[obj_id])} km/h\",\n",
        "                        (cx, cy+20),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 2)\n",
        "\n",
        "    # -------- TRAFFIC ANALYSIS --------\n",
        "    if len(speeds) > 0:\n",
        "\n",
        "        # Compute average vehicle speed\n",
        "        avg_speed = sum(speeds.values()) / len(speeds)\n",
        "\n",
        "        # Display average speed\n",
        "        cv2.putText(frame, f\"Average Speed: {int(avg_speed)} km/h\",\n",
        "                    (20, 40),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)\n",
        "\n",
        "        # Detect congestion condition\n",
        "        if avg_speed < 20:\n",
        "            cv2.putText(frame, \"HEAVY CONGESTION\",\n",
        "                        (20, 80),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)\n",
        "\n",
        "    # -------- VISUAL AIDS --------\n",
        "    # Draw speed measurement reference lines\n",
        "    cv2.line(frame, (0, LINE_Y1), (960, LINE_Y1), (255,0,0), 2)\n",
        "    cv2.line(frame, (0, LINE_Y2), (960, LINE_Y2), (0,255,255), 2)\n",
        "\n",
        "    # Display every 10th frame for performance\n",
        "    if frame_id % 10 == 0:\n",
        "        cv2_imshow(frame)\n",
        "\n",
        "    # Exit on ESC key press\n",
        "    if cv2.waitKey(1) & 0xFF == 27:\n",
        "        break\n",
        "\n",
        "# Release video resources\n",
        "cap.release()\n",
        "\n",
        "# Close OpenCV windows\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "pYMF23FxKbzU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopped Vehicle\n",
        "\n",
        "### Workflow\n",
        "1. Open the video file and read frames sequentially.\n",
        "2. Resize each frame for consistent processing.\n",
        "3. Run YOLO detection to identify vehicles.\n",
        "4. Extract bounding boxes for detected vehicles.\n",
        "5. Update the tracker to obtain vehicle IDs and centroids.\n",
        "6. Measure centroid movement between consecutive frames.\n",
        "7. Determine if movement is below a stopping threshold.\n",
        "8. Count consecutive still frames for each vehicle.\n",
        "9. Classify vehicle as stopped after sufficient still frames.\n",
        "10. Visualize tracking results and stop status on the frame.\n",
        "11. Display processed frames at intervals for efficiency."
      ],
      "metadata": {
        "id": "YS9cqYQZlijh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- IMPORTS ----------------\n",
        "import cv2          # OpenCV for video processing & drawing\n",
        "import time         # Time utilities (not strictly required here)\n",
        "import math         # Distance calculations\n",
        "\n",
        "# ---------------- PARAMETERS ----------------\n",
        "\n",
        "VIDEO_SIZE = (960, 540)   # Standardized frame resolution\n",
        "\n",
        "# YOLO class IDs representing vehicles\n",
        "VEHICLE_CLASSES = [2, 3, 5, 7]   # car, motorcycle, bus, truck\n",
        "\n",
        "STOP_DISTANCE = 6     # Pixel movement tolerance (noise filter)\n",
        "STOP_FRAMES = 20      # Frames required to declare STOPPED\n",
        "\n",
        "# ---------------- MEMORY STRUCTURES ----------------\n",
        "\n",
        "previous_positions = {}   # Stores last centroid per vehicle ID\n",
        "stop_counter = {}         # Counts consecutive still frames\n",
        "frame_id = 0              # Frame counter for display control\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# ---------------- MAIN VIDEO LOOP ----------------\n",
        "while True:\n",
        "\n",
        "    # Read next frame from video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Exit loop if video ends\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Increment frame counter\n",
        "    frame_id += 1\n",
        "\n",
        "    # Resize frame for consistent calculations\n",
        "    frame = cv2.resize(frame, VIDEO_SIZE)\n",
        "\n",
        "    # Run YOLO detection on current frame\n",
        "    results = model(frame)\n",
        "\n",
        "    detections = []   # Store bounding boxes for tracker\n",
        "\n",
        "    # ---------------- YOLO DETECTION ----------------\n",
        "    for r in results:\n",
        "        for box in r.boxes:\n",
        "\n",
        "            # Extract predicted class ID\n",
        "            cls = int(box.cls[0])\n",
        "\n",
        "            # Keep only vehicle detections\n",
        "            if cls in VEHICLE_CLASSES:\n",
        "\n",
        "                # Extract bounding box coordinates\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "\n",
        "                # Add detection for tracking\n",
        "                detections.append([x1, y1, x2, y2])\n",
        "\n",
        "                # Draw bounding box for visualization\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2),\n",
        "                              (0, 255, 0), 2)\n",
        "\n",
        "    # ---------------- TRACKER UPDATE ----------------\n",
        "    # Assign persistent IDs & compute centroids\n",
        "    objects = tracker.update(detections)\n",
        "\n",
        "    # ---------------- STOPPED VEHICLE LOGIC ----------------\n",
        "    for obj_id, (cx, cy) in objects.items():\n",
        "\n",
        "        stopped = False   # Default state\n",
        "\n",
        "        # Check if vehicle existed in previous frame\n",
        "        if obj_id in previous_positions:\n",
        "\n",
        "            # Retrieve previous centroid\n",
        "            px, py = previous_positions[obj_id]\n",
        "\n",
        "            # Compute centroid displacement\n",
        "            distance = math.hypot(cx - px, cy - py)\n",
        "\n",
        "            # If movement is very small → vehicle considered still\n",
        "            if distance < STOP_DISTANCE:\n",
        "                stop_counter[obj_id] = stop_counter.get(obj_id, 0) + 1\n",
        "            else:\n",
        "                # Reset counter if vehicle moves\n",
        "                stop_counter[obj_id] = 0\n",
        "\n",
        "            # Declare STOPPED after enough still frames\n",
        "            if stop_counter[obj_id] >= STOP_FRAMES:\n",
        "                stopped = True\n",
        "\n",
        "        # Store centroid for next frame comparison\n",
        "        previous_positions[obj_id] = (cx, cy)\n",
        "\n",
        "        # ---------------- VISUALIZATION ----------------\n",
        "        # Red → stopped, Green → moving\n",
        "        color = (0, 0, 255) if stopped else (0, 255, 0)\n",
        "\n",
        "        # Draw centroid marker\n",
        "        cv2.circle(frame, (cx, cy), 5, color, -1)\n",
        "\n",
        "        # Display status label\n",
        "        label = f\"STOPPED ID {obj_id}\" if stopped else f\"ID {obj_id}\"\n",
        "\n",
        "        cv2.putText(frame,\n",
        "                    label,\n",
        "                    (cx, cy - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.5,\n",
        "                    (255, 255, 255),\n",
        "                    2)\n",
        "\n",
        "    # ---------------- DISPLAY CONTROL ----------------\n",
        "    # Show only some frames to keep runtime fast\n",
        "    if frame_id % 10 == 0:\n",
        "        cv2_imshow(frame)\n",
        "\n",
        "# Release video resources\n",
        "cap.release()\n",
        "\n",
        "# Close OpenCV windows\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6plVVKeqhRPQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}